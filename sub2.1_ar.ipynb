{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######     The sentence are converted into the 300-D vectors\n",
    "######      Sentence_Vector = the gravity center of the distributed word vectors in the sentence.\n",
    "\n",
    "######                          \\Sigma (word2vec(iword) * \\rho(iword)) \n",
    "######      sen_vec    = ----------------------------------\n",
    "######                             (\\Sigma \\rho(iword))\n",
    "######\n",
    "######       In this version2, \\rho = tf-idf(iword)  while only words in word2vec (GoogleNews trained model) are calculated. \n",
    "######       tensorflow with 2 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import re\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalize the stopwords array\n",
    "stopwords=['a','and','of','to','','\\n']\n",
    "global stopwords\n",
    "filename_train='data/train.csv'\n",
    "filename_test='data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27971\n",
      "27971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the sentence in train data \n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def load_sentence(filename_1,filename_2):\n",
    "    \n",
    "    text_content=[]\n",
    "    file_1=open(filename_1,'r')\n",
    "    for line in islice(file_1,1,None):    # SKIP the head line!!!\n",
    "        text_content.append(line)\n",
    "    file_1.close()   \n",
    "    train_len=len(text_content)\n",
    "    \n",
    "    file_2=open(filename_2,\"r\")\n",
    "    for line in islice(file_2,1,None):    # SKIP the head line!!!\n",
    "        text_content.append(line)\n",
    "    file_2.close()\n",
    "    test_len=len(text_content) - train_len        \n",
    "        \n",
    "    return text_content,train_len,test_len                # NOTE THAT text_content: head line deleted!!!\n",
    "\n",
    "text_content,train_len,test_len=load_sentence(filename_train,filename_test)        \n",
    "        \n",
    "print(len(text_content))\n",
    "print(train_len+test_len)\n",
    "type(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFIDF weight matrix:\n",
      "\n",
      "(27971, 56271)\n",
      "Time consuming: 17.12557101249695\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Get the ifidf value of words in each sentence ( including train and test data)\n",
    "\n",
    "def create_tfidf(text_content):   # text_content : head line already deleted!\n",
    "    \n",
    "    tfidf=TfidfVectorizer(stop_words=stopwords)\n",
    "    weight=tfidf.fit_transform(text_content).toarray()\n",
    "    tf_vocabulary=tfidf.vocabulary_\n",
    "    #iword=tf_vocabulary['leather']\n",
    "\n",
    "    #print ('vocabulary list:\\n')\n",
    "    #for key,value in tfidf.vocabulary_.items():\n",
    "    #    print (key,value)\n",
    "\n",
    "    print('IFIDF weight matrix:\\n')\n",
    "    print (weight.shape)\n",
    "\n",
    "    return tf_vocabulary,weight\n",
    "since=time.time()\n",
    "tf_voca,weight=create_tfidf(text_content)\n",
    "print(\"Time consuming:\", time.time()-since)\n",
    "#iword=tf_voca['leather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52278\n",
      "υπνος\n"
     ]
    }
   ],
   "source": [
    "inverse_tf_voca=dict(zip(tf_voca.values(),tf_voca.keys()))\n",
    "iword=tf_voca['survivor']\n",
    "print(iword)\n",
    "weight[588][iword]\n",
    "for i in range(56270,56271):\n",
    "    print(inverse_tf_voca[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10302734 -0.15234375  0.02587891  0.16503906 -0.16503906  0.06689453\n",
      "  0.29296875 -0.26367188 -0.140625    0.20117188 -0.02624512 -0.08203125\n",
      " -0.02770996 -0.04394531 -0.23535156  0.16992188  0.12890625  0.15722656\n",
      "  0.00756836 -0.06982422 -0.03857422  0.07958984  0.22949219 -0.14355469\n",
      "  0.16796875 -0.03515625  0.05517578  0.10693359  0.11181641 -0.16308594\n",
      " -0.11181641  0.13964844  0.01556396  0.12792969  0.15429688  0.07714844\n",
      "  0.26171875  0.08642578 -0.02514648  0.33398438  0.18652344 -0.20996094\n",
      "  0.07080078  0.02600098 -0.10644531 -0.10253906  0.12304688  0.04711914\n",
      "  0.02209473  0.05834961 -0.10986328  0.14941406 -0.10693359  0.01556396\n",
      "  0.08984375  0.11230469 -0.04370117 -0.11376953 -0.0037384  -0.01818848\n",
      "  0.24316406  0.08447266 -0.07080078  0.18066406  0.03515625 -0.09667969\n",
      " -0.21972656 -0.00328064 -0.03198242  0.18457031  0.28515625 -0.0859375\n",
      " -0.11181641  0.0213623  -0.30664062 -0.09228516 -0.18945312  0.01513672\n",
      "  0.18554688  0.34375    -0.31054688  0.22558594  0.08740234 -0.2265625\n",
      " -0.29492188  0.08251953 -0.38476562  0.25390625  0.26953125  0.06298828\n",
      " -0.00958252  0.23632812 -0.17871094 -0.12451172 -0.17285156 -0.11767578\n",
      "  0.19726562 -0.03466797 -0.10400391 -0.1640625  -0.19726562  0.19824219\n",
      "  0.09521484  0.00561523  0.12597656  0.00073624 -0.0402832  -0.03063965\n",
      "  0.01623535 -0.1640625  -0.22167969  0.171875    0.12011719 -0.01965332\n",
      "  0.4453125   0.06494141  0.05932617 -0.1640625  -0.01367188  0.18945312\n",
      "  0.05566406 -0.05004883 -0.01422119  0.15917969  0.07421875 -0.31640625\n",
      " -0.0534668  -0.02355957 -0.16992188  0.0625     -0.140625   -0.13183594\n",
      " -0.12792969  0.12060547  0.05883789 -0.00055695  0.05761719 -0.08447266\n",
      "  0.16992188  0.13671875 -0.09375     0.08056641 -0.04003906 -0.03759766\n",
      " -0.26367188  0.00662231 -0.01928711  0.09423828 -0.13183594 -0.27929688\n",
      "  0.27734375  0.31835938  0.10058594  0.11425781 -0.27734375  0.11035156\n",
      " -0.06982422 -0.0859375  -0.11132812 -0.27929688 -0.07763672  0.05102539\n",
      " -0.06176758  0.44921875  0.01867676 -0.15039062  0.13671875 -0.15039062\n",
      " -0.2265625   0.11914062  0.06005859 -0.0390625  -0.10839844  0.02905273\n",
      "  0.02331543  0.13183594  0.01000977  0.03149414 -0.12597656 -0.13671875\n",
      " -0.30664062 -0.28515625  0.09863281 -0.00564575 -0.08398438 -0.16015625\n",
      " -0.14453125 -0.18261719  0.10009766  0.04345703  0.10644531  0.16503906\n",
      "  0.06298828  0.17578125  0.15820312  0.125       0.171875   -0.05664062\n",
      " -0.09033203 -0.21289062 -0.0390625  -0.03320312 -0.18652344 -0.18945312\n",
      " -0.01818848 -0.03051758 -0.17675781  0.15234375  0.01953125  0.01696777\n",
      " -0.10839844  0.18066406  0.00497437 -0.11621094 -0.19824219 -0.19140625\n",
      "  0.21386719  0.08984375 -0.16308594 -0.00325012 -0.07128906  0.08251953\n",
      "  0.02160645  0.10107422 -0.29101562 -0.04785156 -0.03369141 -0.14453125\n",
      " -0.06396484 -0.12451172 -0.24804688  0.09277344  0.01165771  0.24707031\n",
      "  0.3828125   0.02429199 -0.12255859  0.00259399 -0.015625    0.00540161\n",
      "  0.04736328 -0.00744629 -0.18261719  0.21972656 -0.16601562  0.359375\n",
      "  0.05297852 -0.03808594 -0.10400391 -0.18457031  0.07470703  0.2734375\n",
      "  0.28320312  0.15234375  0.05126953 -0.15429688 -0.15722656 -0.21582031\n",
      " -0.19824219  0.09960938 -0.09179688  0.07519531 -0.02441406  0.17382812\n",
      " -0.11279297  0.00219727  0.07470703 -0.02307129  0.08642578  0.16601562\n",
      "  0.07763672  0.14746094  0.15429688  0.11425781  0.14550781 -0.24023438\n",
      " -0.2109375   0.07421875 -0.14746094 -0.08642578  0.16113281 -0.07128906\n",
      "  0.109375    0.17675781  0.18554688 -0.15820312 -0.2265625   0.15039062\n",
      " -0.08544922  0.09130859 -0.03198242  0.13476562 -0.15136719 -0.42773438\n",
      " -0.03442383 -0.27929688  0.125      -0.19824219 -0.12304688  0.06494141]\n",
      "time: 36.07217502593994\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the trained model: GoogleNews or Glove or whatever\n",
    "\n",
    "since=time.time()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/miaohan/trained_model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#new_sentence=['favour']\n",
    "#model.train()\n",
    "print(model['love'])\n",
    "print(\"time:\",time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'id08064', '', '', 'One', 'Survivor', 'and', 'Dead', 'Man', 'Found', 'Aboard', '', '', '', 'HPL', '\\n']\n",
      "line 588\n",
      "line 2304\n",
      "line 4558\n",
      "line 6262\n",
      "line 6647\n",
      "line 6936\n",
      "line 9250\n",
      "line 9950\n",
      "line 10542\n",
      "line 11306\n",
      "line 13401\n",
      "line 14012\n",
      "line 18064\n",
      "line 18127\n",
      "line 18501\n",
      "file loaded!\n",
      "computation time: 3.308793067932129\n"
     ]
    }
   ],
   "source": [
    "# Step 4: load the training file into training format\n",
    "\n",
    "def load_train(filename,tf_voca,weight):   # Be Cautious : len(text in filename) = len(weight)+1  \n",
    "\n",
    "    text_content=[]\n",
    "    text_author=[]\n",
    "    tf_word=[]\n",
    "    input_file=open(filename,\"r\")\n",
    "    iline=-1\n",
    "    for line in islice(input_file,1,None):\n",
    "        iline=iline+1\n",
    "        row=re.split(\" |!|\\?|\\.|\\,|\\\"|\\'|;|\\:\", line)    \n",
    "        text_content=row[:]                 # Duplication cuz text_content could be removed in the row loop!\n",
    "        if iline==588:\n",
    "            print(text_content)\n",
    "\n",
    "        \n",
    "        for word in row:                      # strip the stop_words and meaningless str\n",
    "            if word in stopwords:\n",
    "                text_content.remove(word)      \n",
    "        \n",
    "        del text_content[0]                 # Cast the id item !\n",
    " \n",
    "        if text_content[-1] == \"EAP\":\n",
    "            #print(\"EAP HERE!\")\n",
    "            text_author.append([1,0,0])\n",
    "        elif text_content[-1] == \"HPL\":\n",
    "            text_author.append([0,1,0])\n",
    "        elif text_content[-1] == \"MWS\":\n",
    "            text_author.append([0,0,1])   #load Author\n",
    "              \n",
    "        del text_content[-1]                 # Cast the author name after adding to the one-hot text_author!      \n",
    "\n",
    "        vec_sen_line=[0]*300     \n",
    "        mass_sum=0    \n",
    "        for word in text_content:        \n",
    "            try:\n",
    "                    \n",
    "                word_index=tf_voca[word]             \n",
    "                tfidf_value=weight[iline][word_index]\n",
    "                if iline == 588:\n",
    "                    print(text_content)\n",
    "                    print(\"try\",word,\"iline\",iline)\n",
    "                    print(\"tfidf_value\",tfidf_value)\n",
    "\n",
    "                \n",
    "                vec_sen_line=vec_sen_line+np.asarray(model[word]) * tfidf_value  # center of gravity= ({Sigma pos(i)*mass(i)}/{Sigma mass(i)})\n",
    "                mass_sum=mass_sum+tfidf_value \n",
    "            except:\n",
    "                pass\n",
    "                #print(\"UKN word\",word)\n",
    "            #print(\"sum mass\", mass_sum)\n",
    "        #print(\"vec_sen_line type\",type(vec_sen_line))\n",
    "        if mass_sum != 0.0:\n",
    "            \n",
    "            tf_word.append(np.asarray(vec_sen_line)/mass_sum)\n",
    "        elif mass_sum== 0.0:\n",
    "            print(\"line\",iline)\n",
    "            tf_word.append(np.asarray(vec_sen_line))\n",
    "     \n",
    "    #print(text_id[0],text_author[0],tf_word[0])\n",
    "    #print(len(text_id),len(text_author),len(tf_word))\n",
    "   \n",
    "    print(\"file loaded!\")\n",
    "    input_file.close()\n",
    "    return text_author,tf_word\n",
    "\n",
    "\n",
    "since=time.time()\n",
    "text_author,tf_word=load_train(filename_train,tf_voca,weight)\n",
    "print(\"computation time:\",time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id12354\",\"His head of hair would have done honor to a Brutus; nothing could be more richly flowing, or possess a brighter gloss.\",\"MWS\"\n",
      "\n",
      "\"id12465\",\"\"They shout,\" I said, \"because they will soon return to England.\" \"Do you, then, really return?\" \"Alas Yes; I cannot withstand their demands.\",\"HPL\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the itertools -- islice\n",
    "\n",
    "from itertools import islice\n",
    "input_file = open(\"data/test.txt\")\n",
    "for line in islice(input_file, 1, None):\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iline 439\n",
      "iline 1011\n",
      "iline 2777\n",
      "iline 4056\n",
      "iline 4081\n",
      "iline 5068\n",
      "iline 5792\n",
      "iline 8097\n",
      "file loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_test(filename,train_len,tf_voca,weight):\n",
    "    text_id=[]\n",
    "    text_content=[]\n",
    "    tf_word=[]\n",
    "    \n",
    "    file=open(filename,\"r\")\n",
    "    iline=-1\n",
    "    for line in islice(file,1,None):\n",
    "        iline=iline+1\n",
    "        #print(\"line\",line)\n",
    "        row=re.split(\" |!|\\?|\\.|\\,|\\\"|\\'|;|\\:\", line)\n",
    "        text_content=row[:]\n",
    "\n",
    "        # strip the stop_words and meaningless str\n",
    "\n",
    "        for word in row:  \n",
    "            if word in stopwords:\n",
    "                \n",
    "                text_content.remove(word)\n",
    "        #print(text_content)        \n",
    "        \n",
    "        \n",
    "        text_id.append(text_content[0]) # load id\n",
    "        #print(\"text_content[0]\",text_content[0])\n",
    "        del text_content[0]\n",
    " \n",
    "\n",
    "                       \n",
    "        vec_sen_line=[0]*300     \n",
    "        mass_sum=0    \n",
    "        for word in text_content:        \n",
    "            try:\n",
    "                word_index=tf_voca[word]\n",
    "                tfidf_value=weight[iline+train_len][word_index]\n",
    "                \n",
    "                vec_sen_line=vec_sen_line+np.asarray(model[word]) *tfidf_value # center of gravity= ({Sigma pos(i)*mass(i)}/{Sigma mass(i)})\n",
    "                mass_sum=mass_sum+tfidf_value\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"UKN word\",word)\n",
    "                \n",
    "        #print(\"vec_sen_line type\",type(vec_sen_line))\n",
    "        if mass_sum != 0.0  :\n",
    "            tf_word.append(np.asarray(vec_sen_line)/mass_sum)\n",
    "        elif mass_sum == 0.0:\n",
    "            print(\"iline\",iline)\n",
    "            tf_word.append(np.asarray(vec_sen_line))\n",
    "     \n",
    " \n",
    "    print(\"file loaded!\")\n",
    "    file.close()\n",
    "    return text_id,tf_word\n",
    "\n",
    "\n",
    "test_id,test_word=load_test(filename_test,train_len,tf_voca,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "1579\n"
     ]
    }
   ],
   "source": [
    "def train_test(text_author,tf_word):\n",
    "    train_author,train_word=text_author[0:18000],tf_word[0:18000]\n",
    "    test_author,test_word=text_author[18000:],tf_word[18000:]\n",
    "\n",
    "    return train_word,train_author,test_word,test_author\n",
    "a=[]\n",
    "a.append(['Then', 'I', 'told', 'him', 'what', 'I'])\n",
    "a.append(['scars', 'on', 'my', \"ancestor\", 'chest'])\n",
    "\n",
    "\n",
    "train_x,train_y,test_x,test_y = train_test(text_author,tf_word)\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "#print(sen_vec)\n",
    "#print(len(sen_vec))\n",
    "#print(sen_vec[0][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename,tid,prediction):\n",
    "    import csv\n",
    "    with open(filename,\"w\") as csvfile: \n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        #columns_name\n",
    "        writer.writerow([\"id\",\"EAP\",\"HPL\",\"MWS\"])\n",
    "        #写入多行用writerows\n",
    "        for ii in range(len(tid)):\n",
    "            type(prediction[ii])\n",
    "            a=[tid[ii]]+list(prediction[ii])\n",
    "            writer.writerow(a)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "######     2 hidden layers    #####\n",
    "\n",
    "from __future__ import print_function \n",
    "import tensorflow as tf\n",
    "\n",
    "#mnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True) \n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 20\n",
    "display_step = 10\n",
    "print (len(tf_word))  #19579\n",
    "\n",
    "###### tf Graph Input #####\n",
    "x = tf.placeholder(tf.float32, [None, 300]) # word2vec of shape 300 features\n",
    "y = tf.placeholder(tf.float32, [None, 3]) # 3 authors to be classified => 3 classes\n",
    "\n",
    "n_hidden_1 = 20 # 1st layer number of neurons\n",
    "n_hidden_2 = 5 # 2nd layer number of neurons\n",
    "n_input = 300 # word2vec of shape 300 features\n",
    "n_classes = 3 # authors classes 3\n",
    "\n",
    "##### Set model weights #####\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "W = tf.Variable(tf.zeros([300, 3]))\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Construct multi-layers model\n",
    "#logits = multilayer_perceptron(x)\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "#print(pred.get_shape())\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "##### Gradient Descent #####\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "##### Initializing the variables #####\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "total_batch 900\n",
      "epoch 1\n",
      "total_batch 900\n",
      "epoch 2\n",
      "total_batch 900\n",
      "epoch 3\n",
      "total_batch 900\n",
      "epoch 4\n",
      "total_batch 900\n",
      "epoch 5\n",
      "total_batch 900\n",
      "epoch 6\n",
      "total_batch 900\n",
      "epoch 7\n",
      "total_batch 900\n",
      "epoch 8\n",
      "total_batch 900\n",
      "epoch 9\n",
      "total_batch 900\n",
      "Epoch: 0010 cost= 0.770758537\n",
      "epoch 10\n",
      "total_batch 900\n",
      "epoch 11\n",
      "total_batch 900\n",
      "epoch 12\n",
      "total_batch 900\n",
      "epoch 13\n",
      "total_batch 900\n",
      "epoch 14\n",
      "total_batch 900\n",
      "epoch 15\n",
      "total_batch 900\n",
      "epoch 16\n",
      "total_batch 900\n",
      "epoch 17\n",
      "total_batch 900\n",
      "epoch 18\n",
      "total_batch 900\n",
      "epoch 19\n",
      "total_batch 900\n",
      "Epoch: 0020 cost= 0.769894322\n",
      "epoch 20\n",
      "total_batch 900\n",
      "epoch 21\n",
      "total_batch 900\n",
      "epoch 22\n",
      "total_batch 900\n",
      "epoch 23\n",
      "total_batch 900\n",
      "epoch 24\n",
      "total_batch 900\n",
      "epoch 25\n",
      "total_batch 900\n",
      "epoch 26\n",
      "total_batch 900\n",
      "epoch 27\n",
      "total_batch 900\n",
      "epoch 28\n",
      "total_batch 900\n",
      "epoch 29\n",
      "total_batch 900\n",
      "Epoch: 0030 cost= 0.769804073\n",
      "epoch 30\n",
      "total_batch 900\n",
      "epoch 31\n",
      "total_batch 900\n",
      "epoch 32\n",
      "total_batch 900\n",
      "epoch 33\n",
      "total_batch 900\n",
      "epoch 34\n",
      "total_batch 900\n",
      "epoch 35\n",
      "total_batch 900\n",
      "epoch 36\n",
      "total_batch 900\n",
      "epoch 37\n",
      "total_batch 900\n",
      "epoch 38\n",
      "total_batch 900\n",
      "epoch 39\n",
      "total_batch 900\n",
      "Epoch: 0040 cost= 0.769775292\n",
      "epoch 40\n",
      "total_batch 900\n",
      "epoch 41\n",
      "total_batch 900\n",
      "epoch 42\n",
      "total_batch 900\n",
      "epoch 43\n",
      "total_batch 900\n",
      "epoch 44\n",
      "total_batch 900\n",
      "epoch 45\n",
      "total_batch 900\n",
      "epoch 46\n",
      "total_batch 900\n",
      "epoch 47\n",
      "total_batch 900\n",
      "epoch 48\n",
      "total_batch 900\n",
      "epoch 49\n",
      "total_batch 900\n",
      "Epoch: 0050 cost= 0.769759043\n",
      "epoch 50\n",
      "total_batch 900\n",
      "epoch 51\n",
      "total_batch 900\n",
      "epoch 52\n",
      "total_batch 900\n",
      "epoch 53\n",
      "total_batch 900\n",
      "epoch 54\n",
      "total_batch 900\n",
      "epoch 55\n",
      "total_batch 900\n",
      "epoch 56\n",
      "total_batch 900\n",
      "epoch 57\n",
      "total_batch 900\n",
      "epoch 58\n",
      "total_batch 900\n",
      "epoch 59\n",
      "total_batch 900\n",
      "Epoch: 0060 cost= 0.769748395\n",
      "epoch 60\n",
      "total_batch 900\n",
      "epoch 61\n",
      "total_batch 900\n",
      "epoch 62\n",
      "total_batch 900\n",
      "epoch 63\n",
      "total_batch 900\n",
      "epoch 64\n",
      "total_batch 900\n",
      "epoch 65\n",
      "total_batch 900\n",
      "epoch 66\n",
      "total_batch 900\n",
      "epoch 67\n",
      "total_batch 900\n",
      "epoch 68\n",
      "total_batch 900\n",
      "epoch 69\n",
      "total_batch 900\n",
      "Epoch: 0070 cost= 0.769741164\n",
      "epoch 70\n",
      "total_batch 900\n",
      "epoch 71\n",
      "total_batch 900\n",
      "epoch 72\n",
      "total_batch 900\n",
      "epoch 73\n",
      "total_batch 900\n",
      "epoch 74\n",
      "total_batch 900\n",
      "epoch 75\n",
      "total_batch 900\n",
      "epoch 76\n",
      "total_batch 900\n",
      "epoch 77\n",
      "total_batch 900\n",
      "epoch 78\n",
      "total_batch 900\n",
      "epoch 79\n",
      "total_batch 900\n",
      "Epoch: 0080 cost= 0.769736192\n",
      "epoch 80\n",
      "total_batch 900\n",
      "epoch 81\n",
      "total_batch 900\n",
      "epoch 82\n",
      "total_batch 900\n",
      "epoch 83\n",
      "total_batch 900\n",
      "epoch 84\n",
      "total_batch 900\n",
      "epoch 85\n",
      "total_batch 900\n",
      "epoch 86\n",
      "total_batch 900\n",
      "epoch 87\n",
      "total_batch 900\n",
      "epoch 88\n",
      "total_batch 900\n",
      "epoch 89\n",
      "total_batch 900\n",
      "Epoch: 0090 cost= 0.769732767\n",
      "epoch 90\n",
      "total_batch 900\n",
      "epoch 91\n",
      "total_batch 900\n",
      "epoch 92\n",
      "total_batch 900\n",
      "epoch 93\n",
      "total_batch 900\n",
      "epoch 94\n",
      "total_batch 900\n",
      "epoch 95\n",
      "total_batch 900\n",
      "epoch 96\n",
      "total_batch 900\n",
      "epoch 97\n",
      "total_batch 900\n",
      "epoch 98\n",
      "total_batch 900\n",
      "epoch 99\n",
      "total_batch 900\n",
      "Epoch: 0100 cost= 0.769730395\n",
      "Optimization Finished!\n",
      "(?, 3)\n",
      "(?, 3)\n",
      "Accuracy: 0.666244\n",
      "submission [[ 0.14056891  0.12427654  0.73515451]\n",
      " [ 0.57596719  0.36606708  0.05796571]\n",
      " [ 0.09193908  0.87174106  0.03631989]\n",
      " ..., \n",
      " [ 0.81262273  0.0963979   0.09097931]\n",
      " [ 0.10725669  0.10963884  0.78310448]\n",
      " [ 0.28750113  0.58894926  0.12354966]]\n"
     ]
    }
   ],
   "source": [
    "##### Launch the graph #####\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    " \n",
    "     ### Training cycle ###\n",
    "    for epoch in range(training_epochs):\n",
    "            \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(train_x)/batch_size)\n",
    "        print(\"epoch\",epoch)\n",
    "        print(\"total_batch\",total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train_x[i*batch_size:(i+1)*batch_size], train_y[i*batch_size : (i+1)*batch_size]\n",
    "              # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                           y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "         # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    " \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    \n",
    "    ##### Test model#####\n",
    " #   pred=tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) \n",
    "    print(pred.get_shape())\n",
    "    print(y.get_shape())\n",
    "    ##### Calculate accuracy #####     \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_x, y: test_y}))\n",
    "    \n",
    "    \n",
    "    print(\"submission\", pred.eval({x:test_word}))\n",
    "    prediction=pred.eval({x:test_word})\n",
    "    #prediction.get_shape()\n",
    "    filename_save='subm2.1_2lys.csv'\n",
    "    save_file(filename_save,test_id,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
