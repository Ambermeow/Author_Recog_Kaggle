{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######     The sentence are converted into the 300-D vectors\n",
    "######      Sentence_Vector = the gravity center of the distributed word vectors in the sentence.\n",
    "\n",
    "######                          \\Sigma (word2vec(iword) * \\rho(iword)) \n",
    "######      sen_vec    = ----------------------------------\n",
    "######                             (\\Sigma \\rho(iword))\n",
    "######       In this version, \\rho = 1/count(iword)  while tfidf would be more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10302734 -0.15234375  0.02587891  0.16503906 -0.16503906  0.06689453\n",
      "  0.29296875 -0.26367188 -0.140625    0.20117188 -0.02624512 -0.08203125\n",
      " -0.02770996 -0.04394531 -0.23535156  0.16992188  0.12890625  0.15722656\n",
      "  0.00756836 -0.06982422 -0.03857422  0.07958984  0.22949219 -0.14355469\n",
      "  0.16796875 -0.03515625  0.05517578  0.10693359  0.11181641 -0.16308594\n",
      " -0.11181641  0.13964844  0.01556396  0.12792969  0.15429688  0.07714844\n",
      "  0.26171875  0.08642578 -0.02514648  0.33398438  0.18652344 -0.20996094\n",
      "  0.07080078  0.02600098 -0.10644531 -0.10253906  0.12304688  0.04711914\n",
      "  0.02209473  0.05834961 -0.10986328  0.14941406 -0.10693359  0.01556396\n",
      "  0.08984375  0.11230469 -0.04370117 -0.11376953 -0.0037384  -0.01818848\n",
      "  0.24316406  0.08447266 -0.07080078  0.18066406  0.03515625 -0.09667969\n",
      " -0.21972656 -0.00328064 -0.03198242  0.18457031  0.28515625 -0.0859375\n",
      " -0.11181641  0.0213623  -0.30664062 -0.09228516 -0.18945312  0.01513672\n",
      "  0.18554688  0.34375    -0.31054688  0.22558594  0.08740234 -0.2265625\n",
      " -0.29492188  0.08251953 -0.38476562  0.25390625  0.26953125  0.06298828\n",
      " -0.00958252  0.23632812 -0.17871094 -0.12451172 -0.17285156 -0.11767578\n",
      "  0.19726562 -0.03466797 -0.10400391 -0.1640625  -0.19726562  0.19824219\n",
      "  0.09521484  0.00561523  0.12597656  0.00073624 -0.0402832  -0.03063965\n",
      "  0.01623535 -0.1640625  -0.22167969  0.171875    0.12011719 -0.01965332\n",
      "  0.4453125   0.06494141  0.05932617 -0.1640625  -0.01367188  0.18945312\n",
      "  0.05566406 -0.05004883 -0.01422119  0.15917969  0.07421875 -0.31640625\n",
      " -0.0534668  -0.02355957 -0.16992188  0.0625     -0.140625   -0.13183594\n",
      " -0.12792969  0.12060547  0.05883789 -0.00055695  0.05761719 -0.08447266\n",
      "  0.16992188  0.13671875 -0.09375     0.08056641 -0.04003906 -0.03759766\n",
      " -0.26367188  0.00662231 -0.01928711  0.09423828 -0.13183594 -0.27929688\n",
      "  0.27734375  0.31835938  0.10058594  0.11425781 -0.27734375  0.11035156\n",
      " -0.06982422 -0.0859375  -0.11132812 -0.27929688 -0.07763672  0.05102539\n",
      " -0.06176758  0.44921875  0.01867676 -0.15039062  0.13671875 -0.15039062\n",
      " -0.2265625   0.11914062  0.06005859 -0.0390625  -0.10839844  0.02905273\n",
      "  0.02331543  0.13183594  0.01000977  0.03149414 -0.12597656 -0.13671875\n",
      " -0.30664062 -0.28515625  0.09863281 -0.00564575 -0.08398438 -0.16015625\n",
      " -0.14453125 -0.18261719  0.10009766  0.04345703  0.10644531  0.16503906\n",
      "  0.06298828  0.17578125  0.15820312  0.125       0.171875   -0.05664062\n",
      " -0.09033203 -0.21289062 -0.0390625  -0.03320312 -0.18652344 -0.18945312\n",
      " -0.01818848 -0.03051758 -0.17675781  0.15234375  0.01953125  0.01696777\n",
      " -0.10839844  0.18066406  0.00497437 -0.11621094 -0.19824219 -0.19140625\n",
      "  0.21386719  0.08984375 -0.16308594 -0.00325012 -0.07128906  0.08251953\n",
      "  0.02160645  0.10107422 -0.29101562 -0.04785156 -0.03369141 -0.14453125\n",
      " -0.06396484 -0.12451172 -0.24804688  0.09277344  0.01165771  0.24707031\n",
      "  0.3828125   0.02429199 -0.12255859  0.00259399 -0.015625    0.00540161\n",
      "  0.04736328 -0.00744629 -0.18261719  0.21972656 -0.16601562  0.359375\n",
      "  0.05297852 -0.03808594 -0.10400391 -0.18457031  0.07470703  0.2734375\n",
      "  0.28320312  0.15234375  0.05126953 -0.15429688 -0.15722656 -0.21582031\n",
      " -0.19824219  0.09960938 -0.09179688  0.07519531 -0.02441406  0.17382812\n",
      " -0.11279297  0.00219727  0.07470703 -0.02307129  0.08642578  0.16601562\n",
      "  0.07763672  0.14746094  0.15429688  0.11425781  0.14550781 -0.24023438\n",
      " -0.2109375   0.07421875 -0.14746094 -0.08642578  0.16113281 -0.07128906\n",
      "  0.109375    0.17675781  0.18554688 -0.15820312 -0.2265625   0.15039062\n",
      " -0.08544922  0.09130859 -0.03198242  0.13476562 -0.15136719 -0.42773438\n",
      " -0.03442383 -0.27929688  0.125      -0.19824219 -0.12304688  0.06494141]\n",
      "time: 68.15591764450073\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the trained model: GoogleNews or Glove or whatever\n",
    "\n",
    "since=time.time()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/miaohan/trained_model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(model['love'])\n",
    "print(\"time:\",time.time()-since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['a','and','of','to','','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model['favourite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file loaded!\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a list of strings.\n",
    "def load_train(filename):\n",
    "    text_id=[]\n",
    "    text_content=[]\n",
    "    text_author=[]\n",
    "    tf_word=[]\n",
    "    vocal=[]\n",
    "    file=open(filename,\"r\")\n",
    "    for line in file.readlines():\n",
    "        #print(\"line\",line)\n",
    "        row=re.split(\" |!|\\?|\\.|\\,|\\\"|\\'|;|\\:\", line)\n",
    "        text_content=row[:]\n",
    "\n",
    "        # strip the stop_words and meaningless str\n",
    "\n",
    "        for word in row:  \n",
    "            if word in stop_words:\n",
    "                \n",
    "                text_content.remove(word)\n",
    "        #print(text_content)        \n",
    "        \n",
    "        \n",
    "        text_id.append(text_content[0]) # load id\n",
    "        #print(\"text_content[0]\",text_content[0])\n",
    "        del text_content[0]\n",
    " \n",
    "        if text_content[-1] == \"EAP\":\n",
    "            #print(\"EAP HERE!\")\n",
    "            text_author.append([1,0,0])\n",
    "        elif text_content[-1] == \"HPL\":\n",
    "            text_author.append([0,1,0])\n",
    "        elif text_content[-1] == \"MWS\":\n",
    "            text_author.append([0,0,1])   #load Author\n",
    "            \n",
    "        #print(\"text_content[-1]\",text_content[-1])\n",
    "        del text_content[-1]\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "        vec_sen_line=[0]*300     \n",
    "        mass_sum=0    \n",
    "        for word in text_content:        \n",
    "            try:\n",
    "                count_word=text_content.count(word)\n",
    "                vec_sen_line=vec_sen_line+np.asarray(model[word]) / count_word  # center of gravity= ({Sigma pos(i)*mass(i)}/{Sigma mass(i)})\n",
    "                mass_sum=mass_sum+1./count_word\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"UKN word\",word)\n",
    "                \n",
    "        #print(\"vec_sen_line type\",type(vec_sen_line))\n",
    "        tf_word.append(np.asarray(vec_sen_line)/mass_sum)\n",
    "     \n",
    "    #print(text_id[0],text_author[0],tf_word[0])\n",
    "    #print(len(text_id),len(text_author),len(tf_word))\n",
    "    del text_id[0]\n",
    "    del tf_word[0]\n",
    "    print(\"file loaded!\")\n",
    "    file.close()\n",
    "    return text_id,text_author,tf_word\n",
    "\n",
    "\n",
    "\n",
    "since=time.time()\n",
    "#filename='/home/miaohan/NLP/data/test.txt'\n",
    "filename='/home/miaohan/NLP/data/train.csv'\n",
    "text_id,text_author,tf_word=load_train(filename)\n",
    "#text_id[-10:]\n",
    "#text_content[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id [ 0.27929688 -0.08447266 -0.04150391  0.12011719 -0.01794434  0.12353516\n",
      "  0.1328125  -0.08251953  0.11572266  0.10693359 -0.08056641 -0.01397705\n",
      " -0.13671875  0.11230469 -0.0546875   0.109375    0.12060547  0.06396484\n",
      " -0.02001953 -0.41992188  0.18652344  0.14550781 -0.08837891 -0.23828125\n",
      " -0.21386719 -0.24902344 -0.06884766  0.10693359  0.20996094  0.04589844\n",
      " -0.15527344 -0.29882812 -0.11376953 -0.02416992  0.01477051  0.01489258\n",
      " -0.06835938  0.33203125 -0.10888672 -0.14160156  0.05639648  0.05078125\n",
      "  0.19921875  0.40820312  0.04833984  0.07714844  0.06689453 -0.171875\n",
      " -0.28515625 -0.203125   -0.2109375  -0.18164062  0.03369141 -0.21777344\n",
      "  0.25976562  0.15234375  0.13964844 -0.22167969 -0.1484375   0.03808594\n",
      "  0.02990723 -0.10644531 -0.10595703  0.11181641 -0.02770996  0.19726562\n",
      " -0.28320312  0.08886719  0.06347656  0.21386719  0.16308594  0.08300781\n",
      " -0.03088379  0.09716797  0.10253906 -0.17089844  0.22265625  0.38085938\n",
      "  0.23535156 -0.10839844  0.08642578  0.06494141  0.21972656  0.02978516\n",
      "  0.359375    0.00282288  0.03564453 -0.15332031  0.07666016  0.13378906\n",
      " -0.03369141 -0.10839844 -0.01031494 -0.22070312 -0.05810547 -0.078125\n",
      " -0.09570312 -0.03613281  0.14160156  0.17578125 -0.19433594  0.06152344\n",
      " -0.08300781  0.05273438  0.06982422  0.32226562  0.02722168 -0.01245117\n",
      " -0.08300781  0.19824219 -0.08544922  0.14550781  0.04541016 -0.10400391\n",
      "  0.34570312  0.02416992  0.12158203 -0.25585938  0.28710938  0.17675781\n",
      "  0.10791016 -0.23144531 -0.34179688  0.20605469  0.09814453 -0.17089844\n",
      " -0.31640625 -0.10253906 -0.14648438  0.10839844 -0.296875   -0.11474609\n",
      " -0.11767578  0.00897217  0.05151367  0.05200195 -0.05322266  0.10449219\n",
      "  0.16015625 -0.2578125   0.15527344 -0.33789062 -0.02099609 -0.18066406\n",
      "  0.06982422 -0.08447266  0.02819824  0.12695312 -0.08642578 -0.06396484\n",
      "  0.23339844  0.1875      0.05175781 -0.03564453  0.27734375 -0.48046875\n",
      " -0.22363281  0.125      -0.23535156  0.0625     -0.11132812  0.41796875\n",
      "  0.17285156 -0.01159668  0.125      -0.06176758  0.03637695 -0.1875\n",
      " -0.30078125 -0.14746094 -0.03088379  0.11035156 -0.22558594 -0.16601562\n",
      "  0.11425781 -0.23632812  0.02148438 -0.12988281 -0.23242188 -0.16992188\n",
      " -0.22558594 -0.2734375   0.00686646  0.04370117  0.203125    0.06640625\n",
      " -0.20605469  0.25585938  0.01251221  0.11279297  0.06738281  0.02929688\n",
      " -0.19140625 -0.17480469 -0.00854492  0.28710938  0.11621094 -0.4921875\n",
      "  0.08398438  0.03491211 -0.06835938 -0.12695312 -0.2109375  -0.03637695\n",
      " -0.02185059  0.06933594  0.0859375  -0.06542969  0.02160645 -0.0378418\n",
      "  0.17773438  0.00854492  0.08007812 -0.12597656 -0.12011719  0.04492188\n",
      "  0.03320312 -0.05249023 -0.11523438  0.0189209  -0.21777344  0.15234375\n",
      "  0.05981445 -0.28515625  0.06347656  0.08154297 -0.10449219  0.12060547\n",
      "  0.0390625   0.17285156  0.11132812 -0.17285156 -0.04272461 -0.02478027\n",
      "  0.04614258 -0.16992188  0.03320312  0.08886719  0.24707031 -0.00491333\n",
      "  0.02819824  0.05859375  0.00193787 -0.05126953  0.328125    0.13671875\n",
      "  0.10644531  0.39648438 -0.22851562  0.234375    0.08642578  0.16699219\n",
      " -0.24121094  0.06933594 -0.03295898 -0.44335938  0.06884766 -0.34375\n",
      " -0.03491211 -0.02880859  0.17578125  0.06591797 -0.03015137 -0.171875\n",
      " -0.04858398  0.08837891  0.296875   -0.30859375 -0.13574219 -0.44921875\n",
      "  0.0177002   0.234375   -0.03320312  0.09228516  0.05712891 -0.04760742\n",
      "  0.06445312 -0.03320312 -0.09228516  0.18554688 -0.0703125   0.23632812\n",
      " -0.18164062  0.21386719  0.28515625  0.02648926  0.09814453  0.11767578\n",
      "  0.04663086  0.2734375  -0.04394531 -0.07666016  0.01269531 -0.23046875\n",
      "  0.04077148 -0.06835938  0.11767578 -0.22070312  0.00878906 -0.00051498]\n",
      "8393 8393\n",
      "file loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_test(filename):\n",
    "    text_id=[]\n",
    "    text_content=[]\n",
    "    tf_word=[]\n",
    "    \n",
    "    file=open(filename,\"r\")\n",
    "    for line in file.readlines():\n",
    "        #print(\"line\",line)\n",
    "        row=re.split(\" |!|\\?|\\.|\\,|\\\"|\\'|;|\\:\", line)\n",
    "        text_content=row[:]\n",
    "\n",
    "        # strip the stop_words and meaningless str\n",
    "\n",
    "        for word in row:  \n",
    "            if word in stop_words:\n",
    "                \n",
    "                text_content.remove(word)\n",
    "        #print(text_content)        \n",
    "        \n",
    "        \n",
    "        text_id.append(text_content[0]) # load id\n",
    "        #print(\"text_content[0]\",text_content[0])\n",
    "        del text_content[0]\n",
    " \n",
    "\n",
    "                       \n",
    "        vec_sen_line=[0]*300     \n",
    "        mass_sum=0    \n",
    "        for word in text_content:        \n",
    "            try:\n",
    "                count_word=text_content.count(word)\n",
    "                vec_sen_line=vec_sen_line+np.asarray(model[word]) / count_word  # center of gravity= ({Sigma pos(i)*mass(i)}/{Sigma mass(i)})\n",
    "                mass_sum=mass_sum+1./count_word\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"UKN word\",word)\n",
    "                \n",
    "        #print(\"vec_sen_line type\",type(vec_sen_line))\n",
    "        tf_word.append(np.asarray(vec_sen_line)/mass_sum)\n",
    "     \n",
    "    #print(text_id[0],tf_word[0])\n",
    "    #print(len(text_id),len(tf_word))\n",
    "    del text_id[0]\n",
    "    del tf_word[0]\n",
    "    print(\"file loaded!\")\n",
    "    file.close()\n",
    "    return text_id,tf_word\n",
    "\n",
    "\n",
    "filename='/home/miaohan/NLP/data/test.csv'\n",
    "test_id,test_word=load_test(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "1579\n"
     ]
    }
   ],
   "source": [
    "def train_test(text_author,tf_word):\n",
    "    dindex=18000\n",
    "    train_author,train_word=text_author[0:dindex],tf_word[0:dindex]\n",
    "    test_author,test_word=text_author[dindex:],tf_word[dindex:]\n",
    "\n",
    "    return train_word,train_author,test_word,test_author\n",
    "a=[]\n",
    "a.append(['Then', 'I', 'told', 'him', 'what', 'I'])\n",
    "a.append(['scars', 'on', 'my', \"ancestor\", 'chest'])\n",
    "\n",
    "\n",
    "train_x,train_y,test_x,test_y = train_test(text_author,tf_word)\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "#print(sen_vec)\n",
    "#print(len(sen_vec))\n",
    "#print(sen_vec[0][0])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename,tid,prediction):\n",
    "    import csv\n",
    "    with open(filename,\"w\") as csvfile: \n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        #columns_name\n",
    "        writer.writerow([\"id\",\"EAP\",\"HPL\",\"MWS\"])\n",
    "        #写入多行用writerows\n",
    "        for ii in range(len(tid)):\n",
    "            type(prediction[ii])\n",
    "            a=[tid[ii]]+list(prediction[ii])\n",
    "            writer.writerow(a)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "(?, 3)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import tensorflow as tf\n",
    "\n",
    "#mnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True) \n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 20\n",
    "display_step = 1\n",
    "print (len(text_id))  #19579\n",
    "\n",
    "###### tf Graph Input #####\n",
    "x = tf.placeholder(tf.float32, [None, 300]) # word2vec of shape 300 features\n",
    "y = tf.placeholder(tf.float32, [None, 3]) # 3 authors to be classified => 3 classes\n",
    "\n",
    "##### Set model weights #####\n",
    "W = tf.Variable(tf.zeros([300, 3]))\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    " \n",
    " ##### Construct model #####\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "print(pred.get_shape())\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "##### Gradient Descent #####\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "##### Initializing the variables #####\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "total_batch 900\n",
      "Epoch: 0001 cost= 0.828629934\n",
      "epoch 1\n",
      "total_batch 900\n",
      "Epoch: 0002 cost= 0.737129878\n",
      "epoch 2\n",
      "total_batch 900\n",
      "Epoch: 0003 cost= 0.715645902\n",
      "epoch 3\n",
      "total_batch 900\n",
      "Epoch: 0004 cost= 0.705181956\n",
      "epoch 4\n",
      "total_batch 900\n",
      "Epoch: 0005 cost= 0.699023883\n",
      "epoch 5\n",
      "total_batch 900\n",
      "Epoch: 0006 cost= 0.695026120\n",
      "epoch 6\n",
      "total_batch 900\n",
      "Epoch: 0007 cost= 0.692269420\n",
      "epoch 7\n",
      "total_batch 900\n",
      "Epoch: 0008 cost= 0.690289791\n",
      "epoch 8\n",
      "total_batch 900\n",
      "Epoch: 0009 cost= 0.688826217\n",
      "epoch 9\n",
      "total_batch 900\n",
      "Epoch: 0010 cost= 0.687720174\n",
      "epoch 10\n",
      "total_batch 900\n",
      "Epoch: 0011 cost= 0.686869795\n",
      "epoch 11\n",
      "total_batch 900\n",
      "Epoch: 0012 cost= 0.686206786\n",
      "epoch 12\n",
      "total_batch 900\n",
      "Epoch: 0013 cost= 0.685683808\n",
      "epoch 13\n",
      "total_batch 900\n",
      "Epoch: 0014 cost= 0.685267211\n",
      "epoch 14\n",
      "total_batch 900\n",
      "Epoch: 0015 cost= 0.684932486\n",
      "epoch 15\n",
      "total_batch 900\n",
      "Epoch: 0016 cost= 0.684661505\n",
      "epoch 16\n",
      "total_batch 900\n",
      "Epoch: 0017 cost= 0.684440642\n",
      "epoch 17\n",
      "total_batch 900\n",
      "Epoch: 0018 cost= 0.684259516\n",
      "epoch 18\n",
      "total_batch 900\n",
      "Epoch: 0019 cost= 0.684110122\n",
      "epoch 19\n",
      "total_batch 900\n",
      "Epoch: 0020 cost= 0.683986247\n",
      "epoch 20\n",
      "total_batch 900\n",
      "Epoch: 0021 cost= 0.683883012\n",
      "epoch 21\n",
      "total_batch 900\n",
      "Epoch: 0022 cost= 0.683796544\n",
      "epoch 22\n",
      "total_batch 900\n",
      "Epoch: 0023 cost= 0.683723788\n",
      "epoch 23\n",
      "total_batch 900\n",
      "Epoch: 0024 cost= 0.683662260\n",
      "epoch 24\n",
      "total_batch 900\n",
      "Epoch: 0025 cost= 0.683610004\n",
      "epoch 25\n",
      "total_batch 900\n",
      "Epoch: 0026 cost= 0.683565395\n",
      "epoch 26\n",
      "total_batch 900\n",
      "Epoch: 0027 cost= 0.683527128\n",
      "epoch 27\n",
      "total_batch 900\n",
      "Epoch: 0028 cost= 0.683494144\n",
      "epoch 28\n",
      "total_batch 900\n",
      "Epoch: 0029 cost= 0.683465575\n",
      "epoch 29\n",
      "total_batch 900\n",
      "Epoch: 0030 cost= 0.683440685\n",
      "epoch 30\n",
      "total_batch 900\n",
      "Epoch: 0031 cost= 0.683418886\n",
      "epoch 31\n",
      "total_batch 900\n",
      "Epoch: 0032 cost= 0.683399691\n",
      "epoch 32\n",
      "total_batch 900\n",
      "Epoch: 0033 cost= 0.683382700\n",
      "epoch 33\n",
      "total_batch 900\n",
      "Epoch: 0034 cost= 0.683367563\n",
      "epoch 34\n",
      "total_batch 900\n",
      "Epoch: 0035 cost= 0.683354003\n",
      "epoch 35\n",
      "total_batch 900\n",
      "Epoch: 0036 cost= 0.683341770\n",
      "epoch 36\n",
      "total_batch 900\n",
      "Epoch: 0037 cost= 0.683330689\n",
      "epoch 37\n",
      "total_batch 900\n",
      "Epoch: 0038 cost= 0.683320579\n",
      "epoch 38\n",
      "total_batch 900\n",
      "Epoch: 0039 cost= 0.683311294\n",
      "epoch 39\n",
      "total_batch 900\n",
      "Epoch: 0040 cost= 0.683302733\n",
      "epoch 40\n",
      "total_batch 900\n",
      "Epoch: 0041 cost= 0.683294784\n",
      "epoch 41\n",
      "total_batch 900\n",
      "Epoch: 0042 cost= 0.683287374\n",
      "epoch 42\n",
      "total_batch 900\n",
      "Epoch: 0043 cost= 0.683280420\n",
      "epoch 43\n",
      "total_batch 900\n",
      "Epoch: 0044 cost= 0.683273868\n",
      "epoch 44\n",
      "total_batch 900\n",
      "Epoch: 0045 cost= 0.683267666\n",
      "epoch 45\n",
      "total_batch 900\n",
      "Epoch: 0046 cost= 0.683261767\n",
      "epoch 46\n",
      "total_batch 900\n",
      "Epoch: 0047 cost= 0.683256137\n",
      "epoch 47\n",
      "total_batch 900\n",
      "Epoch: 0048 cost= 0.683250741\n",
      "epoch 48\n",
      "total_batch 900\n",
      "Epoch: 0049 cost= 0.683245564\n",
      "epoch 49\n",
      "total_batch 900\n",
      "Epoch: 0050 cost= 0.683240566\n",
      "epoch 50\n",
      "total_batch 900\n",
      "Epoch: 0051 cost= 0.683235746\n",
      "epoch 51\n",
      "total_batch 900\n",
      "Epoch: 0052 cost= 0.683231061\n",
      "epoch 52\n",
      "total_batch 900\n",
      "Epoch: 0053 cost= 0.683226522\n",
      "epoch 53\n",
      "total_batch 900\n",
      "Epoch: 0054 cost= 0.683222100\n",
      "epoch 54\n",
      "total_batch 900\n",
      "Epoch: 0055 cost= 0.683217796\n",
      "epoch 55\n",
      "total_batch 900\n",
      "Epoch: 0056 cost= 0.683213596\n",
      "epoch 56\n",
      "total_batch 900\n",
      "Epoch: 0057 cost= 0.683209483\n",
      "epoch 57\n",
      "total_batch 900\n",
      "Epoch: 0058 cost= 0.683205465\n",
      "epoch 58\n",
      "total_batch 900\n",
      "Epoch: 0059 cost= 0.683201526\n",
      "epoch 59\n",
      "total_batch 900\n",
      "Epoch: 0060 cost= 0.683197667\n",
      "epoch 60\n",
      "total_batch 900\n",
      "Epoch: 0061 cost= 0.683193880\n",
      "epoch 61\n",
      "total_batch 900\n",
      "Epoch: 0062 cost= 0.683190155\n",
      "epoch 62\n",
      "total_batch 900\n",
      "Epoch: 0063 cost= 0.683186496\n",
      "epoch 63\n",
      "total_batch 900\n",
      "Epoch: 0064 cost= 0.683182906\n",
      "epoch 64\n",
      "total_batch 900\n",
      "Epoch: 0065 cost= 0.683179371\n",
      "epoch 65\n",
      "total_batch 900\n",
      "Epoch: 0066 cost= 0.683175894\n",
      "epoch 66\n",
      "total_batch 900\n",
      "Epoch: 0067 cost= 0.683172472\n",
      "epoch 67\n",
      "total_batch 900\n",
      "Epoch: 0068 cost= 0.683169097\n",
      "epoch 68\n",
      "total_batch 900\n",
      "Epoch: 0069 cost= 0.683165780\n",
      "epoch 69\n",
      "total_batch 900\n",
      "Epoch: 0070 cost= 0.683162512\n",
      "epoch 70\n",
      "total_batch 900\n",
      "Epoch: 0071 cost= 0.683159284\n",
      "epoch 71\n",
      "total_batch 900\n",
      "Epoch: 0072 cost= 0.683156112\n",
      "epoch 72\n",
      "total_batch 900\n",
      "Epoch: 0073 cost= 0.683152983\n",
      "epoch 73\n",
      "total_batch 900\n",
      "Epoch: 0074 cost= 0.683149900\n",
      "epoch 74\n",
      "total_batch 900\n",
      "Epoch: 0075 cost= 0.683146865\n",
      "epoch 75\n",
      "total_batch 900\n",
      "Epoch: 0076 cost= 0.683143867\n",
      "epoch 76\n",
      "total_batch 900\n",
      "Epoch: 0077 cost= 0.683140915\n",
      "epoch 77\n",
      "total_batch 900\n",
      "Epoch: 0078 cost= 0.683137999\n",
      "epoch 78\n",
      "total_batch 900\n",
      "Epoch: 0079 cost= 0.683135123\n",
      "epoch 79\n",
      "total_batch 900\n",
      "Epoch: 0080 cost= 0.683132293\n",
      "epoch 80\n",
      "total_batch 900\n",
      "Epoch: 0081 cost= 0.683129500\n",
      "epoch 81\n",
      "total_batch 900\n",
      "Epoch: 0082 cost= 0.683126743\n",
      "epoch 82\n",
      "total_batch 900\n",
      "Epoch: 0083 cost= 0.683124033\n",
      "epoch 83\n",
      "total_batch 900\n",
      "Epoch: 0084 cost= 0.683121354\n",
      "epoch 84\n",
      "total_batch 900\n",
      "Epoch: 0085 cost= 0.683118710\n",
      "epoch 85\n",
      "total_batch 900\n",
      "Epoch: 0086 cost= 0.683116110\n",
      "epoch 86\n",
      "total_batch 900\n",
      "Epoch: 0087 cost= 0.683113540\n",
      "epoch 87\n",
      "total_batch 900\n",
      "Epoch: 0088 cost= 0.683111005\n",
      "epoch 88\n",
      "total_batch 900\n",
      "Epoch: 0089 cost= 0.683108512\n",
      "epoch 89\n",
      "total_batch 900\n",
      "Epoch: 0090 cost= 0.683106048\n",
      "epoch 90\n",
      "total_batch 900\n",
      "Epoch: 0091 cost= 0.683103625\n",
      "epoch 91\n",
      "total_batch 900\n",
      "Epoch: 0092 cost= 0.683101226\n",
      "epoch 92\n",
      "total_batch 900\n",
      "Epoch: 0093 cost= 0.683098864\n",
      "epoch 93\n",
      "total_batch 900\n",
      "Epoch: 0094 cost= 0.683096531\n",
      "epoch 94\n",
      "total_batch 900\n",
      "Epoch: 0095 cost= 0.683094237\n",
      "epoch 95\n",
      "total_batch 900\n",
      "Epoch: 0096 cost= 0.683091967\n",
      "epoch 96\n",
      "total_batch 900\n",
      "Epoch: 0097 cost= 0.683089745\n",
      "epoch 97\n",
      "total_batch 900\n",
      "Epoch: 0098 cost= 0.683087541\n",
      "epoch 98\n",
      "total_batch 900\n",
      "Epoch: 0099 cost= 0.683085371\n",
      "epoch 99\n",
      "total_batch 900\n",
      "Epoch: 0100 cost= 0.683083230\n",
      "Optimization Finished!\n",
      "(?, 3)\n",
      "(?, 3)\n",
      "Accuracy: 0.706143\n",
      "submission [[ 0.2186624   0.21352524  0.56781232]\n",
      " [ 0.68589413  0.26517093  0.04893494]\n",
      " [ 0.05469434  0.93431997  0.01098569]\n",
      " ..., \n",
      " [ 0.75795215  0.16690092  0.07514697]\n",
      " [ 0.09677303  0.05220097  0.85102606]\n",
      " [ 0.14255798  0.79858434  0.05885767]]\n"
     ]
    }
   ],
   "source": [
    "##### Launch the graph #####\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    " \n",
    "     ### Training cycle ###\n",
    "    for epoch in range(training_epochs):\n",
    "            \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(train_x)/batch_size)\n",
    "        print(\"epoch\",epoch)\n",
    "        print(\"total_batch\",total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train_x[i*batch_size:(i+1)*batch_size], train_y[i*batch_size : (i+1)*batch_size]\n",
    "              # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                           y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "         # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    " \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    \n",
    "    ##### Test model#####\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) \n",
    "    print(pred.get_shape())\n",
    "    print(y.get_shape())\n",
    "    ##### Calculate accuracy #####     \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_x, y: test_y}))\n",
    "    print(\"submission\", pred.eval({x:test_word}))\n",
    "    prediction=pred.eval({x:test_word})\n",
    "    #prediction.get_shape()\n",
    "    filename='submission.csv'\n",
    "    save_file(filename,test_id,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
